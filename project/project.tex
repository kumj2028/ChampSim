\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage{svg}

\usetikzlibrary{shapes,positioning,shapes.gates.logic}

\tikzset{ell/.style={circle,draw,minimum height=0.2cm,minimum width=0.2cm,inner sep=0.15cm}}
\tikzset{rec/.style={rectangle,draw,minimum height=0.5cm,minimum width=0.5cm,inner sep=0.2cm}}
\tikzset{trp/.style={trapezium,draw,trapezium left angle=120, trapezium right angle=120, minimum height=0.5cm}}


%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Branch Prediction Models Exploration\\
\thanks{}
}
\author{\IEEEauthorblockN{Daniela Trevina}
\IEEEauthorblockA{\textit{Department of EECS} \\
\textit{Texas A\&M University-Kingsville}\\
Kingsville, USA \\
daniela.trevino@students.tamuk.edu}
\and
\IEEEauthorblockN{Daniela Lopez}
\IEEEauthorblockA{\textit{Department of EECS} \\
\textit{Texas A\&M University-Kingsville}\\
Kingsville, USA \\
daniela.lopez@students.tamuk.edu}
\and
\IEEEauthorblockN{Mengxiang Jiang}
\IEEEauthorblockA{\textit{Department of EECS} \\
\textit{Texas A\&M University-Kingsville}\\
Kingsville, USA \\
mengxiang.jiang@students.tamuk.edu}
\and
\IEEEauthorblockN{Samah Allahyani}
\IEEEauthorblockA{\textit{Department of EECS} \\
\textit{Texas A\&M University-Kingsville}\\
Kingsville, USA \\
samah.allahyani@students.tamuk.edu}
\and
\IEEEauthorblockN{Ugochukwu Onyeakazi}
\IEEEauthorblockA{\textit{Department of EECS} \\
\textit{Texas A\&M University-Kingsville}\\
Kingsville, USA \\
ugochukwu.onyeakazi@students.tamuk.edu}
}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction} \label{sec:introduction}
Branch prediction is a crucial component in modern computer architecture to improve performance and energy efficiency. When executing a program, the processor encounters branches in the control flow, where it must decide which path to take based on a conditional statement. Incorrect branch predictions result in wasted computation cycles and negatively impact the overall performance of the system. To mitigate this issue, branch predictors are employed to predict the outcome of the conditional statements, and enable the processor to pre-fetch instructions from the predicted path, reducing the overhead of branching.

Over the years, a variety of branch prediction models have been developed to improve the accuracy of predictions. These models range from static branch predictors that use heuristics to predict the outcome of branches, to more sophisticated dynamic branch predictors (all models tested in this paper are dynamic) that rely on past history to make predictions. Furthermore, the emergence of  machine learning and artificial intelligence has led to the development of more advanced branch prediction techniques.

The aim of this paper is to explore the various branch prediction models and compare their performances. We will examine the underlying principles of each model, the algorithms used to make predictions and their implementations in a simulated processor. We will also discuss the challenges associated with branch prediction, such as the trade-off between prediction accuracy and hardware complexity.

\section{Models} \label{sec:models}
In this section, we will cover the various branch predictor models used in the simulation.

\subsection{Bimodal Predictor} \label{ssec:bimodal}
This model was first proposed by James E. Smith as an improvement on the single-bit last-time predictor\cite{smith1981}. This is a local history predictor in the sense that the Program Counter (PC) is used to differentiate branches and their past outcomes. It stores two bits in the branch target buffer (BTB) indicating whether this particular branch was taken or not taken in the last two times encountered. These two bits encode a state machine with four states shown in Fig. 
\ref{fig:bimodal}. 
\begin{figure}
    \centering
    \begin{tikzpicture}[>=stealth]
		\node[ell,align=center] (st) at (0,3) {Strongly\\ Taken\\11};
        \node[ell,align=center] (wt) at (3,3) {Weakly\\ Taken\\10};
        \node[ell,align=center] (snt) at (3,0) {Strongly\\ Not Taken\\00};
        \node[ell,align=center] (wnt) at (0,0) {Weakly\\ Not Taken\\01};

        \draw [->] (st) to [loop above, looseness=5]node[]{1} (st);
        \draw [->] (st) to [bend left=20]node[above]{0} (wt);
        \draw [->] (wt) to [bend left=20]node[above]{1} (st);
        \draw [->] (wt) to [bend right=10]node[above]{0} (wnt);
        \draw [->] (wnt) to [bend right=10]node[above]{1} (wt);
        \draw [->] (wnt) to [bend left=20]node[above]{0} (snt);
        \draw [->] (snt) to [bend left=20]node[above]{1} (wnt);
        \draw [->] (snt) to [loop below, looseness=5]node[]{0} (snt);
    \end{tikzpicture}
    \caption{Bimodal Predictor}
	\label{fig:bimodal}
\end{figure}
The \textit{Strongly Taken} and \textit{Weakly Taken} states will predict \textit{taken} for the current branch, while the \textit{Strongly Not Taken} and \textit{Weakly Not Taken} states will predict \textit{not taken}.

\subsection{Gshare Predictor} \label{ssec:gshare}
This model was first proposed by Scott McFarling as an improvement on the global history predictor\cite{mcfarling1993}. A normal global history predictor uses the single history from the Global History Register (GHR) to index into the Pattern History Table (PHT) in order to predict all branch outcomes regardless of the PC. Gshare XORs the PC with the GHR for indexing into the PHT, which adds some local context information for the predictor to take into account.

\subsection{Perceptron Predictor} \label{ssec:perceptron}
This model is a single-layer version of an artificial neural network that can identify and classify patterns, first applied to branch prediction by Daniel A. Jimen\'ez and Calvin Lin\cite{jimenez2001dynamic}. In the model shown in Fig. \ref{fig:perceptron} are the input vector ($x$), the weight vector ($w$), and the output ($y$).
\begin{figure}
    \centering
    \begin{tikzpicture}[>=stealth]
		\node[ell] (x0) at (2,6) {$1$};
        \node[ell] (x1) at (0,4) {$x_1$};
        \node[ell] (xi) at (0,2) {$x_i$};
        \node[ell] (xh) at (0,0) {$x_h$};
        \node[ell] (y) at (6,3) {$y$};

        \draw [->] (x0) to [bend left]node[above]{$w_0$} (y);
		\draw [->] (x1) to [bend left=10]node[above]{$w_1$} (y);
		\draw [->] (xi) to [bend right=10]node[above]{$w_i$} (y);
		\draw [->] (xh) to [bend right]node[above]{$w_h$} (y);
    \end{tikzpicture}
    \caption{Perceptron Predictor}
	\label{fig:perceptron}
\end{figure}
The inputs correspond to values taken from the global history register, with the exception of $x_0$, the bias, always set to $1$. The output's sign determines the prediction. If the sign is negative, the branch is not taken, otherwise it is taken. Equation \eqref{eqn:1} shows the calculation of the output:
\begin{equation}
    y = w_o + \sum_{i=1}^{h}{x_iw_i}
    \label{eqn:1}
\end{equation}

The weights of the perceptron are trained using the algorithm below:
\begin{lstlisting}
if sgn(y) != t or abs(y) <= theta  then
    for i := 0 to n do
        w[i] := w[i] + t*x[i]
    end
end
\end{lstlisting}
$t$ is 1 if the branch is actually taken or -1 if not. $\theta$ is the threshold to determine when training should stop. $w_i$ is incremented if $t$ and $x_i$ agree, and decremented if they disagree.

\subsection{Hashed Perceptron Predictor} \label{ssec:hp}
This predictor is based off of the perceptron branch predictor from the previous section(\ref{ssec:perceptron}) but with a few important changes. The first change is instead of a single table for the weights, multiple independently indexed tables of perceptron weights are used, introduced by Jimen\'ez \cite{jimenez2003}.  The second is using the hash of the global history rather than just the global history for indexing in order to reduce the number of independent tables, contemporaneously by Seznec \cite{seznec2004}, Tarjan and Skadron\cite{skadron2004}, and Loh and Jimen\'ez\cite{loh2005reducing}.  The third is instead of using a fixed global history length, exponentially increasing global history lengths are used for indexing, by Seznec\cite{seznec2004gehl}\cite{seznec2005analysis}. This idea is actually one of the key parts of the TAGE predictor described in the next section(\ref{ssec:tage}). The last is to dynamically adjust the $\theta$ value for the training, also by Seznec\cite{seznec2004gehl}\cite{seznec2005analysis}.

\subsection{TAGE Predictor} \label{ssec:tage}
TAGE stands for TAgged GEometric history length branch predictor, and it was introduced by Andr\'e Seznec\cite{seznec2006case}. The first key idea, as mentioned earlier, is using exponentially increasing global history lengths for indexing into multiple tables, in this case PHTs while for the hashed perceptron it was the weights. The second is to intelligently allocate the PHT entries to different branches. Fig. \ref{fig:tage} shows a 3-component TAGE predictor, while the TAGE predictor evaluated in our simulations has 13 components.
\begin{figure}
    \centering
    \begin{tikzpicture}[>=stealth]
		\node[] (pc0) at (0,11) {PC};
        \node[] (pc1) at (2,12) {PC};
        \node[] (pc2) at (5,12) {PC};
        \node[] (h1) at (3.5,12) {h[0:L(1)]};
        \node[] (h2) at (6.5,12) {h[0:L(2)]};
        \node[rec] (hash1) at (3, 11) {hash};
        \node[rec] (hash2) at (6, 11) {hash};
        \node[rec, rotate=90] (bp) at (0,9) {base predictor};
        \node[rec, rotate=90, align=center] (p1) at (3,9) {\underline{predictor}\\\underline{ useful }\\tag};
        \node[rec, rotate=90, align=center] (p2) at (6,9) {\underline{predictor}\\\underline{ useful }\\tag};
        \node[rec] (eq1) at (3,7) {=};
        \node[rec] (eq2) at (6,7) {=};
        \node[trp] (t0) at (2,6) {};
        \node[trp] (t1) at (5,5) {};
        \node[] (out) at (5,4) {};

        \draw [->] (pc0) to []node[]{} (bp);
        \draw [->] (bp) to [bend right=40]node[]{} (t0);
        \draw [->] (pc1) to []node[]{} (hash1);
        \draw [->] (h1) to []node[]{} (hash1);
        \draw [->] (hash1) to []node[]{} (p1);
        \draw [->] (hash1) to [bend left=70]node[]{} (eq1);
        \draw [->] (p1) to []node[]{} (eq1);
        \draw [->] (p1) to [bend right=10]node[]{} (t0);
        \draw [->] (eq1) to [bend left=50]node[]{} (t0);
        \draw [->] (pc2) to []node[]{} (hash2);
        \draw [->] (h2) to []node[]{} (hash2);
        \draw [->] (hash2) to []node[]{} (p2);
        \draw [->] (hash2) to [bend left=70]node[]{} (eq2);
        \draw [->] (p2) to []node[]{} (eq2);
        \draw [->] (p2) to [bend right=10]node[]{} (t1);
        \draw [->] (eq2) to [bend left=50]node[]{} (t1);
        \draw [->] (t0) to [bend right=20]node[]{} (t1);
        \draw [->] (t1) to []node[]{} (out);

    \end{tikzpicture}
    \caption{3-Component TAGE Predictor}
	\label{fig:tage}
\end{figure}

\subsection{L-TAGE Predictor} \label{ssec:ltage}
This predictor is based off of the TAGE predictor from the previous section(\ref{ssec:tage}), but with the addition of a loop predictor. It was introduced by Seznec and won the 2nd Championship Branch Prediction competition\cite{seznec2004}. A loop predictor first predicts the limit on how many iterations the loop will execute. It keeps track of the number of times the loop as executed, and as long as this count is below the limit, it will predict taken. 

\section{Methodology} \label{sec:methodology}
The study conducts experiments using the ChampSim Simulator. Each predictor is implemented in the branch prediction unit of the simulator. The predictors are then tested on application benchmarks from Standard Performance Evaluation Corporation (SPEC) suite of benchmarks. The performance of each predictor is recorded and presented in the next section (\ref{sec:results}).


\bibliographystyle{plain}
\bibliography{project}

\vspace{12pt}

\end{document}
